#! /usr/bin/env bash
# Patch: -pro_powerpc_Add_E500_SMP_support
# Date: Wed Apr  9 10:00:50 2008
# Source: Freescale
# MR: 26902
# Type: Enhancement
# Disposition: merged from MPC8572DS_20071204-ltib
# Signed-off-by: Randy Vinson <rvinson@mvista.com>
# Description:
# Add E500 SMP support
# 
# Signed-off-by: Kumar Gala <kumar.gala@freescale.com>
# Signed-off-by: Haiying Wang <haiying.wang@freescale.com>
# 

PATCHNUM=1015
LSPINFO=include/linux/lsppatchlevel.h
TMPFILE=/tmp/mvl_patch_$$

function dopatch() {
    patch $* >${TMPFILE} 2>&1 <<"EOF"
Source: Freescale
MR: 26902
Type: Enhancement
Disposition: merged from MPC8572DS_20071204-ltib
Signed-off-by: Randy Vinson <rvinson@mvista.com>
Description:
Add E500 SMP support

Signed-off-by: Kumar Gala <kumar.gala@freescale.com>
Signed-off-by: Haiying Wang <haiying.wang@freescale.com>

Index: linux-2.6.18/arch/powerpc/kernel/head_fsl_booke.S
===================================================================
--- linux-2.6.18.orig/arch/powerpc/kernel/head_fsl_booke.S
+++ linux-2.6.18/arch/powerpc/kernel/head_fsl_booke.S
@@ -21,7 +21,7 @@
  * 	   	debbie_chu@mvista.com
  *    Copyright 2002-2004 MontaVista Software, Inc.
  *      PowerPC 44x support, Matt Porter <mporter@kernel.crashing.org>
- *    Copyright 2004 Freescale Semiconductor, Inc
+ *    Copyright 2004-2007 Freescale Semiconductor, Inc
  *      PowerPC e500 modifications, Kumar Gala <galak@kernel.crashing.org>
  *
  * This program is free software; you can redistribute  it and/or modify it
@@ -89,6 +89,7 @@ _ENTRY(_start);
  * if needed
  */
 
+_ENTRY(__early_start)
 /* 1. Find the index of the entry we're executing in */
 	bl	invstr				/* Find our address */
 invstr:	mflr	r6				/* Make it accessible */
@@ -148,15 +149,12 @@ skpinv:	addi	r6,r6,1				/* Increment */
 	/* Invalidate TLB0 */
 	li      r6,0x04
 	tlbivax 0,r6
-#ifdef CONFIG_SMP
-	tlbsync
-#endif
+	TLBSYNC
+
 	/* Invalidate TLB1 */
 	li      r6,0x0c
 	tlbivax 0,r6
-#ifdef CONFIG_SMP
-	tlbsync
-#endif
+	TLBSYNC
 	msync
 
 /* 3. Setup a temp mapping and jump to it */
@@ -213,9 +211,7 @@ skpinv:	addi	r6,r6,1				/* Increment */
 	/* Invalidate TLB1 */
 	li      r9,0x0c
 	tlbivax 0,r9
-#ifdef CONFIG_SMP
-	tlbsync
-#endif
+	TLBSYNC
 	msync
 
 /* 6. Setup KERNELBASE mapping in TLB1[0] */
@@ -256,9 +252,7 @@ skpinv:	addi	r6,r6,1				/* Increment */
 	/* Invalidate TLB1 */
 	li      r9,0x0c
 	tlbivax 0,r9
-#ifdef CONFIG_SMP
-	tlbsync
-#endif
+	TLBSYNC
 	msync
 
 	/* Establish the interrupt vector offsets */
@@ -322,6 +316,15 @@ skpinv:	addi	r6,r6,1				/* Increment */
 	mtspr	SPRN_DBSR,r2
 #endif
 
+#ifdef CONFIG_SMP
+	/* Check to see if we're the second processor, and jump
+	 * to the secondary_start code if so
+	 */
+	mfspr	r24,SPRN_PIR
+	cmpwi	r24,0
+	bne	__secondary_start
+#endif
+
 	/*
 	 * This is where the main kernel code starts.
 	 */
@@ -378,34 +381,57 @@ skpinv:	addi	r6,r6,1				/* Increment */
 
 /* Macros to hide the PTE size differences
  *
- * FIND_PTE -- walks the page tables given EA & pgdir pointer
+ * FIND_PTE_ADDR -- walks the page tables given EA & pgdir pointer
  *   r10 -- EA of fault
  *   r11 -- PGDIR pointer
  *   r12 -- free
  *   label 2: is the bailout case
  *
  * if we find the pte (fall through):
- *   r11 is low pte word
  *   r12 is pointer to the pte
  */
 #ifdef CONFIG_PTE_64BIT
-#define PTE_FLAGS_OFFSET	4
-#define FIND_PTE	\
+#  define FIND_PTE_ADDR \
 	rlwinm 	r12, r10, 13, 19, 29;	/* Compute pgdir/pmd offset */	\
 	lwzx	r11, r12, r11;		/* Get pgd/pmd entry */		\
 	rlwinm.	r12, r11, 0, 0, 20;	/* Extract pt base address */	\
-	beq	2f;			/* Bail if no table */		\
-	rlwimi	r12, r10, 23, 20, 28;	/* Compute pte address */	\
-	lwz	r11, 4(r12);		/* Get pte entry */
+	rlwimi	r12, r10, 23, 20, 28;	/* Compute pte address */
+
+#  ifdef CONFIG_SMP
+#    define LWARX_PTE \
+	li	r11, 4;							\
+	lwarx	r11, r11, r12;		/* lwarx pte */
+
+#    define STWCX_PTE \
+	addi	r12, r12, 4;	\
+	stwcx.	r11, 0, r12;	\
+	addi	r12, r12, -4;	\
 #else
-#define PTE_FLAGS_OFFSET	0
-#define FIND_PTE	\
-	rlwimi	r11, r10, 12, 20, 29;	/* Create L1 (pgdir/pmd) address */	\
-	lwz	r11, 0(r11);		/* Get L1 entry */			\
-	rlwinm.	r12, r11, 0, 0, 19;	/* Extract L2 (pte) base address */	\
-	beq	2f;			/* Bail if no table */			\
-	rlwimi	r12, r10, 22, 20, 29;	/* Compute PTE address */		\
-	lwz	r11, 0(r12);		/* Get Linux PTE */
+#    define LWARX_PTE \
+	lwz	r11, 4(r12)
+
+#    define STWCX_PTE \
+	stw	r11, 4(r12)
+#endif /* CONFIG_SMP */
+
+#else /* 32-bit PTEs */
+#  define FIND_PTE_ADDR	\
+	rlwinm	r12, r10, 12, 20, 29;	/* Compute pgdir/pmd offset */	\
+	lwzx	r11, r12, r11;		/* Get L1 entry */		\
+	rlwinm.	r12, r11, 0, 0, 19;	/* Extract pte base address */	\
+	rlwimi	r12, r10, 22, 20, 29;	/* Compute PTE address */	\
+
+#  ifdef CONFIG_SMP
+#    define LWARX_PTE \
+	lwarx	r11, 0, r12;		/* lwarx pte */
+#    define STWCX_PTE \
+	stwcx.	r11, 0, r12;
+#  else
+#    define LWARX_PTE \
+	lwz	r11, 0(r12);
+#    define STWCX_PTE \
+	stw	r11, 0(r12);
+#  endif
 #endif
 
 /*
@@ -471,38 +497,69 @@ interrupt_base:
 	mfspr	r11,SPRN_SPRG3
 	lwz	r11,PGDIR(r11)
 4:
-	FIND_PTE
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if there's no entry */
+
+5:
+	LWARX_PTE
 
 	/* Are _PAGE_USER & _PAGE_RW set & _PAGE_HWWRITE not? */
 	andi.	r13, r11, _PAGE_RW|_PAGE_USER|_PAGE_HWWRITE
 	cmpwi	0, r13, _PAGE_RW|_PAGE_USER
 	bne	2f			/* Bail if not */
 
-	/* Update 'changed'. */
-	ori	r11, r11, _PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_HWWRITE
-	stw	r11, PTE_FLAGS_OFFSET(r12) /* Update Linux page table */
-
-	/* MAS2 not updated as the entry does exist in the tlb, this
-	   fault taken to detect state transition (eg: COW -> DIRTY)
-	 */
-	andi.	r11, r11, _PAGE_HWEXEC
-	rlwimi	r11, r11, 31, 27, 27	/* SX <- _PAGE_HWEXEC */
-	ori     r11, r11, (MAS3_UW|MAS3_SW|MAS3_UR|MAS3_SR)@l /* set static perms */
-
 	/* update search PID in MAS6, AS = 0 */
-	mfspr	r12, SPRN_PID0
-	slwi	r12, r12, 16
-	mtspr	SPRN_MAS6, r12
+	mfspr	r13, SPRN_PID0
+	slwi	r13, r13, 16
+	mtspr	SPRN_MAS6, r13
 
-	/* find the TLB index that caused the fault.  It has to be here. */
+	/* find the TLB index that caused the fault. */
 	tlbsx	0, r10
 
+#ifdef CONFIG_SMP
+	/*
+	 * It's possible another processor kicked out the entry
+	 * before we did our tlbsx, so check if we hit
+	 */
+	mfspr	r13, SPRN_MAS1
+	rlwinm.	r13,r13, 0, 0, 0;	/* Check the Valid bit */
+	beq	2f
+#endif /* CONFIG_SMP */
+
+	/*
+	 * MAS2 not updated as the entry does exist in the tlb, this
+	 * fault taken to detect state transition (eg: COW -> DIRTY)
+	 */
+	andi.	r13, r11, _PAGE_HWEXEC
+	rlwimi	r13, r13, 31, 27, 27	/* SX <- _PAGE_HWEXEC */
+	ori	r13, r13, (MAS3_UW|MAS3_SW|MAS3_UR|MAS3_SR)@l /* set static perms */
+
 	/* only update the perm bits, assume the RPN is fine */
-	mfspr	r12, SPRN_MAS3
-	rlwimi	r12, r11, 0, 20, 31
-	mtspr	SPRN_MAS3,r12
+	mfspr	r10, SPRN_MAS3
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS3,r10
 	tlbwe
 
+	/* Update 'changed'. */
+	ori	r11, r11, _PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_HWWRITE
+	STWCX_PTE	/* r11 and r12 must be PTE and &PTE */
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we invalidate the entry we just wrote,
+	 * and start over
+	 */
+	beq	6f
+	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b		/* Try again */
+#endif	/* CONFIG_SMP */
+6:
+
 	/* Done...restore registers and get out of here.  */
 	mfspr	r11, SPRN_SPRG7R
 	mtcr	r11
@@ -513,6 +570,13 @@ interrupt_base:
 	rfi			/* Force context change */
 
 2:
+#ifdef CONFIG_SMP
+	/* Clear the reservation */
+	lis	r11, dummy_stwcx@h
+	ori	r11,r11, dummy_stwcx@l
+	stwcx.	r11, 0, r11
+#endif
+
 	/*
 	 * The bailout.  Restore registers to pre-exception conditions
 	 * and call the heavyweights to help us out.
@@ -603,18 +667,116 @@ interrupt_base:
 	lwz	r11,PGDIR(r11)
 
 4:
-	FIND_PTE
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if no entry */
+
+5:
+	LWARX_PTE
+
 	andi.	r13, r11, _PAGE_PRESENT	/* Is the page present? */
 	beq	2f			/* Bail if not present */
 
+	/*
+	 * We set execute, because we don't have the granularity to
+	 * properly set this at the page level (Linux problem).
+	 * Many of these bits are software only.  Bits we don't set
+	 * here we (properly should) assume have the appropriate value.
+	 */
+
+	mfspr	r10, SPRN_MAS2
 #ifdef CONFIG_PTE_64BIT
-	lwz	r13, 0(r12)
+	rlwimi	r10, r11, 26, 24, 31	/* extract ...WIMGE from pte */
+#else
+	rlwimi	r10, r11, 26, 27, 31	/* extract WIMGE from pte */
+#endif
+#ifdef CONFIG_SMP
+	ori	r10, r10, (MAS2_M)@l
 #endif
+	mtspr	SPRN_MAS2, r10
+
+	bge	5, 11f
+
+	/* is user addr */
+	andi.	r13, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
+	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
+	srwi	r10, r13, 1
+	or	r10, r13, r10	/* Copy user perms into supervisor */
+	iseleq	r10, 0, r10
+	b	22f
+
+	/* is kernel addr */
+11:	rlwinm	r10, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
+	ori	r10, r10, (MAS3_SX | MAS3_SR)
+
+#ifdef CONFIG_PTE_64BIT
+22:	lwz	r13, 0(r12)
+	rlwimi	r10, r13, 24, 0, 7	/* grab RPN[32:39] */
+	rlwimi	r10, r11, 24, 8, 19	/* grab RPN[40:51] */
+	mtspr	SPRN_MAS3, r10
+BEGIN_FTR_SECTION
+	srwi	r10, r13, 8		/* grab RPN[8:31] */
+	mtspr	SPRN_MAS7, r10
+END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
+#else
+22:	rlwimi	r10, r11, 0, 0, 19	/* Extract RPN from PTE and
+					 * merge with perms */
+	mtspr	SPRN_MAS3, r10
+#endif
+#ifdef CONFIG_E200
+	/* Round robin TLB1 entries assignment */
+	mfspr	r13, SPRN_MAS0
+
+	/* Extract TLB1CFG(NENTRY) */
+	mfspr	r10, SPRN_TLB1CFG
+	andi.	r10, r10, 0xfff
+
+	/* Extract MAS0(NV) */
+	andi.	r13, r13, 0xfff
+	addi	r13, r13, 1
+	cmpw	0, r13, r10
+
+	/* check if we need to wrap */
+	blt	77f
+
+	/* wrap back to first free tlbcam entry */
+	lis	r13, tlbcam_index@ha
+	lwz	r13, tlbcam_index@l(r13)
+77:
+	mfspr	r10, SPRN_MAS0
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS0,r10
+#endif /* CONFIG_E200 */
+
+	tlbwe
+
 	ori	r11, r11, _PAGE_ACCESSED
-	stw	r11, PTE_FLAGS_OFFSET(r12)
+	STWCX_PTE
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we need to invalidate
+	 * the TLB entry, and re-lwarx the pte
+	 */
+	beq	88f
+	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b
+#endif	/* CONFIG_SMP */
+
+88:
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi					/* Force context change */
 
-	 /* Jump to common tlb load */
-	b	finish_tlb_load
 2:
 	/* The bailout.  Restore registers to pre-exception conditions
 	 * and call the heavyweights to help us out.
@@ -664,18 +826,115 @@ interrupt_base:
 	lwz	r11,PGDIR(r11)
 
 4:
-	FIND_PTE
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if no entry */
+
+5:
+	LWARX_PTE
+
 	andi.	r13, r11, _PAGE_PRESENT	/* Is the page present? */
 	beq	2f			/* Bail if not present */
 
+	/*
+	 * We set execute, because we don't have the granularity to
+	 * properly set this at the page level (Linux problem).
+	 * Many of these bits are software only.  Bits we don't set
+	 * here we (properly should) assume have the appropriate value.
+	 */
+
+	mfspr	r10, SPRN_MAS2
+#ifdef CONFIG_PTE_64BIT
+	rlwimi	r10, r11, 26, 24, 31	/* extract ...WIMGE from pte */
+#else
+	rlwimi	r10, r11, 26, 27, 31	/* extract WIMGE from pte */
+#endif
+#ifdef CONFIG_SMP
+	ori	r10, r10, (MAS2_M)@l
+#endif
+	mtspr	SPRN_MAS2, r10
+
+	bge	5, 11f
+
+	/* is user addr */
+	andi.	r13, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
+	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
+	srwi	r10, r13, 1
+	or	r10, r13, r10	/* Copy user perms into supervisor */
+	iseleq	r10, 0, r10
+	b	22f
+
+	/* is kernel addr */
+11:	rlwinm	r10, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
+	ori	r10, r10, (MAS3_SX | MAS3_SR)
+
 #ifdef CONFIG_PTE_64BIT
-	lwz	r13, 0(r12)
+22:	lwz	r13, 0(r12)
+	rlwimi	r10, r13, 24, 0, 7	/* grab RPN[32:39] */
+	rlwimi	r10, r11, 24, 8, 19	/* grab RPN[40:51] */
+	mtspr	SPRN_MAS3, r10
+BEGIN_FTR_SECTION
+	srwi	r10, r13, 8		/* grab RPN[8:31] */
+	mtspr	SPRN_MAS7, r10
+END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
+#else
+22:	rlwimi	r10, r11, 0, 0, 19	/* Extract RPN from PTE and
+					 * merge with perms */
+	mtspr	SPRN_MAS3, r10
 #endif
+#ifdef CONFIG_E200
+	/* Round robin TLB1 entries assignment */
+	mfspr	r13, SPRN_MAS0
+
+	/* Extract TLB1CFG(NENTRY) */
+	mfspr	r10, SPRN_TLB1CFG
+	andi.	r10, r10, 0xfff
+
+	/* Extract MAS0(NV) */
+	andi.	r13, r13, 0xfff
+	addi	r13, r13, 1
+	cmpw	0, r13, r10
+
+	/* check if we need to wrap */
+	blt	77f
+
+	/* wrap back to first free tlbcam entry */
+	lis	r13, tlbcam_index@ha
+	lwz	r13, tlbcam_index@l(r13)
+77:
+	mfspr	r10, SPRN_MAS0
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS0,r10
+#endif /* CONFIG_E200 */
+
+	tlbwe
+
 	ori	r11, r11, _PAGE_ACCESSED
-	stw	r11, PTE_FLAGS_OFFSET(r12)
+	STWCX_PTE
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we need to invalidate
+	 * the TLB entry, and re-lwarx the pte
+	 */
+	beq	88f
+	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b
+#endif	/* CONFIG_SMP */
 
-	/* Jump to common TLB load point */
-	b	finish_tlb_load
+88:
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi					/* Force context change */
 
 2:
 	/* The bailout.  Restore registers to pre-exception conditions
@@ -740,96 +999,6 @@ data_access:
 	addi	r3,r1,STACK_FRAME_OVERHEAD
 	EXC_XFER_EE_LITE(0x0300, CacheLockingException)
 
-/*
-
- * Both the instruction and data TLB miss get to this
- * point to load the TLB.
- * 	r10 - EA of fault
- * 	r11 - TLB (info from Linux PTE)
- * 	r12, r13 - available to use
- * 	CR5 - results of addr < TASK_SIZE
- *	MAS0, MAS1 - loaded with proper value when we get here
- *	MAS2, MAS3 - will need additional info from Linux PTE
- *	Upon exit, we reload everything and RFI.
- */
-finish_tlb_load:
-	/*
-	 * We set execute, because we don't have the granularity to
-	 * properly set this at the page level (Linux problem).
-	 * Many of these bits are software only.  Bits we don't set
-	 * here we (properly should) assume have the appropriate value.
-	 */
-
-	mfspr	r12, SPRN_MAS2
-#ifdef CONFIG_PTE_64BIT
-	rlwimi	r12, r11, 26, 24, 31	/* extract ...WIMGE from pte */
-#else
-	rlwimi	r12, r11, 26, 27, 31	/* extract WIMGE from pte */
-#endif
-	mtspr	SPRN_MAS2, r12
-
-	bge	5, 1f
-
-	/* is user addr */
-	andi.	r12, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
-	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
-	srwi	r10, r12, 1
-	or	r12, r12, r10	/* Copy user perms into supervisor */
-	iseleq	r12, 0, r12
-	b	2f
-
-	/* is kernel addr */
-1:	rlwinm	r12, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
-	ori	r12, r12, (MAS3_SX | MAS3_SR)
-
-#ifdef CONFIG_PTE_64BIT
-2:	rlwimi	r12, r13, 24, 0, 7	/* grab RPN[32:39] */
-	rlwimi	r12, r11, 24, 8, 19	/* grab RPN[40:51] */
-	mtspr	SPRN_MAS3, r12
-BEGIN_FTR_SECTION
-	srwi	r10, r13, 8		/* grab RPN[8:31] */
-	mtspr	SPRN_MAS7, r10
-END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
-#else
-2:	rlwimi	r11, r12, 0, 20, 31	/* Extract RPN from PTE and merge with perms */
-	mtspr	SPRN_MAS3, r11
-#endif
-#ifdef CONFIG_E200
-	/* Round robin TLB1 entries assignment */
-	mfspr	r12, SPRN_MAS0
-
-	/* Extract TLB1CFG(NENTRY) */
-	mfspr	r11, SPRN_TLB1CFG
-	andi.	r11, r11, 0xfff
-
-	/* Extract MAS0(NV) */
-	andi.	r13, r12, 0xfff
-	addi	r13, r13, 1
-	cmpw	0, r13, r11
-	addi	r12, r12, 1
-
-	/* check if we need to wrap */
-	blt	7f
-
-	/* wrap back to first free tlbcam entry */
-	lis	r13, tlbcam_index@ha
-	lwz	r13, tlbcam_index@l(r13)
-	rlwimi	r12, r13, 0, 20, 31
-7:
-	mtspr   SPRN_MAS0,r12
-#endif /* CONFIG_E200 */
-
-	tlbwe
-
-	/* Done...restore registers and get out of here.  */
-	mfspr	r11, SPRN_SPRG7R
-	mtcr	r11
-	mfspr	r13, SPRN_SPRG5R
-	mfspr	r12, SPRN_SPRG4R
-	mfspr	r11, SPRN_SPRG1
-	mfspr	r10, SPRN_SPRG0
-	rfi					/* Force context change */
-
 #ifdef CONFIG_SPE
 /* Note that the SPE support is closely modeled after the AltiVec
  * support.  Changes to one are likely to be applicable to the
@@ -1032,6 +1201,157 @@ _GLOBAL(set_context)
 	isync			/* Force context change */
 	blr
 
+#ifdef CONFIG_SMP
+/* To boot secondary cpus, we need a place for them to start up.
+ * Normally, they start at 0xfffffffc, but that's usually the
+ * firmware, and we don't want to have to run the firmware again.
+ * Instead, the primary cpu will set the BPTR to point here to
+ * this page.  We then set up the core, and head to
+ * start_secondary.  Note that this means that the code below
+ * must never exceed 1023 instructions (the branch at the end
+ * would then be the 1024th).
+ */
+	.globl	__secondary_start_page
+	.align	12
+__secondary_start_page:
+/* First do some preliminary setup */
+	lis	r3, HID0_EMCP@h /* Enable machine check */
+	ori	r3,r3,0x4000	/* Enable the Time Base */
+#ifdef CONFIG_PHYS_64BIT
+	ori	r3,r3,0x0080	/* Enable MAS7 updates */
+#endif
+	mtspr	SPRN_HID0,r3
+
+	mfspr	r3, SPRN_TCR
+	oris	r3,r3, TCR_DIE@h /* Enable the Decrementer Interrupt */
+	mtspr	SPRN_TCR,r3
+
+	li	r3,0x3000
+	mtspr	SPRN_HID1,r3
+
+	/* Enable branch prediction */
+	li	r3,0x201
+	mtspr	SPRN_BUCSR,r3
+
+	/* Enable/invalidate the I-Cache */
+	mfspr	r0,SPRN_L1CSR1
+	ori	r0,r0,(L1CSR1_ICFI|L1CSR1_ICE)
+	oris	r0,r0,L1CSR1_ICPE@h
+	mtspr	SPRN_L1CSR1,r0
+	isync
+
+	/* Enable/invalidate the D-Cache */
+	mfspr	r0,SPRN_L1CSR0
+	ori	r0,r0,(L1CSR0_DCFI|L1CSR0_DCE)
+	oris	r0,r0,L1CSR0_DCPE@h
+	msync
+	isync
+	mtspr	SPRN_L1CSR0,r0
+	isync
+
+
+/*
+ * Coming here, we know the cpu has one TLB mapping in TLB1[0]
+ * which maps 0xfffff000-0xffffffff one-to-one.  We set up a
+ * second mapping that maps 0 to 0 for 16M, and then we jump to
+ * __early_start
+ */
+	lis	r6,0x1001	/* Set TLB=1 and ESEL=1 */
+	mtspr	SPRN_MAS0,r6
+	lis	r6,(MAS1_VALID|MAS1_IPROT)@h
+	ori	r6,r6,(MAS1_TSIZE(BOOKE_PAGESZ_16M))@l
+	mtspr	SPRN_MAS1,r6
+	li	r6,0
+	mtspr	SPRN_MAS2,r6	/* EPN is 0 */
+	lis	r7,0	/* RPN is 0*/
+	ori	r7,r7,(MAS3_SX|MAS3_SW|MAS3_SR)
+	mtspr	SPRN_MAS3,r7
+	tlbwe
+
+/* Now we have another mapping for this page, so we jump to that
+ * mapping
+ */
+	mfmsr	r4
+	lis	r7,__early_start@h
+	ori	r7,r7,__early_start@l
+	rlwinm	r7,r7,0,20,31
+	mtspr	SPRN_SRR0,r7
+	mtspr	SPRN_SRR1,r4
+	rfi
+
+/* When we get here, r24 needs to hold the CPU # */
+	.globl __secondary_start
+__secondary_start:
+	lis	r3,__secondary_hold_acknowledge@h
+	ori	r3,r3,__secondary_hold_acknowledge@l
+	stw	r24,0(r3)
+
+	li	r3,0
+	mr	r4,r24		/* Why? */
+	bl	call_setup_cpu
+
+	/* r26 should be safe, here */
+	lis	r26, tlbcam_index@ha
+	lwz	r26, tlbcam_index@l(r26)
+
+	/* Load CAM 0 */
+	li	r3,0
+	bl	loadcam_entry
+
+	/* Load CAM 1, if it's set */
+	li	r3,1
+	cmpw	r3,r26
+	bgt	1f
+	bl	loadcam_entry
+1:
+	/* Load CAM 2, if it's set */
+	li	r3,2
+	cmpw	r3,r26
+	bgt	2f
+	bl	loadcam_entry
+2:
+
+	/* get current_thread_info and current */
+	lis	r1,secondary_ti@ha
+	lwz	r1,secondary_ti@l(r1)
+	lwz	r2,TI_TASK(r1)
+
+	/* stack */
+	addi	r1,r1,THREAD_SIZE-STACK_FRAME_OVERHEAD
+	li	r0,0
+	stw	r0,0(r1)
+
+	/* ptr to current thread */
+	addi	r4,r2,THREAD	/* address of our thread_struct */
+	mtspr	SPRN_SPRG3,r4
+
+	/* Setup the defaults for TLB entries */
+	li	r4,(MAS4_TSIZED(BOOKE_PAGESZ_4K))@l
+	mtspr	SPRN_MAS4,r4
+
+	/* Jump to start_secondary */
+	lis	r4,MSR_KERNEL@h
+	ori	r4,r4,MSR_KERNEL@l
+	lis	r3,start_secondary@h
+	ori	r3,r3,start_secondary@l
+	mtspr	SPRN_SRR0,r3
+	mtspr	SPRN_SRR1,r4
+	sync
+	rfi
+	sync
+
+	.globl __secondary_hold_acknowledge
+__secondary_hold_acknowledge:
+	.long	-1
+
+	/* Fill in the empty space.  The actual reset vector is
+	 * the last word of the page */
+__secondary_start_code_end:
+	.space 4092 - (__secondary_start_code_end - __secondary_start_page)
+__secondary_reset_vector:
+	b	__secondary_start_page
+#endif
+
 /*
  * We put a few things here that have to be page-aligned. This stuff
  * goes at the beginning of the data segment, which is page-aligned.
@@ -1057,6 +1377,14 @@ exception_stack_bottom:
 exception_stack_top:
 
 /*
+ * We need a place to stwcx. to when we want to clear a reservation
+ * without knowing where the original was.
+ */
+	.globl	dummy_stwcx
+dummy_stwcx:
+	.space	4
+
+/*
  * This space gets a copy of optional info passed to us by the bootstrap
  * which is used to pass parameters into the kernel like root=/dev/sda1, etc.
  */
Index: linux-2.6.18/arch/powerpc/kernel/misc_32.S
===================================================================
--- linux-2.6.18.orig/arch/powerpc/kernel/misc_32.S
+++ linux-2.6.18/arch/powerpc/kernel/misc_32.S
@@ -246,6 +246,12 @@ _GLOBAL(real_writeb)
 
 #endif /* CONFIG_40x */
 
+#ifdef CONFIG_SMP
+	.globl	ivax_lock
+ivax_lock:
+	.space	4
+#endif	/* CONFIG_SMP */
+
 /*
  * Flush MMU TLB
  */
@@ -269,6 +275,18 @@ _GLOBAL(_tlbia)
 
 	isync
 #elif defined(CONFIG_FSL_BOOKE)
+#ifdef CONFIG_SMP
+	/* For mutual exclusion for invalidate */
+	lis	r8, ivax_lock@h
+	ori	r8, r8, ivax_lock@l
+11:	lwarx	r5, 0, r8
+	cmpwi	r5, 0
+	bne	11b
+	ori	r5,r5,1
+	stwcx.	r5, 0, r8
+	bne	11b
+#endif	/* CONFIG_SMP */
+
 	/* Invalidate all entries in TLB0 */
 	li	r3, 0x04
 	tlbivax	0,3
@@ -282,8 +300,10 @@ _GLOBAL(_tlbia)
 	li	r3, 0x1c
 	tlbivax	0,3
 	msync
+	TLBSYNC
 #ifdef CONFIG_SMP
-	tlbsync
+	li	r0, 0
+	stw	r0, 0(r8)
 #endif /* CONFIG_SMP */
 #else /* !(CONFIG_40x || CONFIG_44x || CONFIG_FSL_BOOKE) */
 #if defined(CONFIG_SMP)
@@ -373,6 +393,18 @@ _GLOBAL(_tlbie)
 	isync
 10:
 #elif defined(CONFIG_FSL_BOOKE)
+#ifdef CONFIG_SMP
+	/* For mutual exclusion for invalidate */
+	lis	r8, ivax_lock@h
+	ori	r8, r8, ivax_lock@l
+11:	lwarx	r5, 0, r8
+	cmpwi	r5, 0
+	bne	11b
+	ori	r5,r5,1
+	stwcx.	r5, 0, r8
+	bne	11b
+#endif	/* CONFIG_SMP */
+
 	rlwinm	r4, r3, 0, 0, 19
 	ori	r5, r4, 0x08	/* TLBSEL = 1 */
 	ori	r6, r4, 0x10	/* TLBSEL = 2 */
@@ -382,8 +414,10 @@ _GLOBAL(_tlbie)
 	tlbivax	0, r6
 	tlbivax	0, r7
 	msync
+	TLBSYNC
 #if defined(CONFIG_SMP)
-	tlbsync
+	li	r0, 0
+	stw	r0, 0(r8)
 #endif /* CONFIG_SMP */
 #else /* !(CONFIG_40x || CONFIG_44x || CONFIG_FSL_BOOKE) */
 #if defined(CONFIG_SMP)
Index: linux-2.6.18/arch/powerpc/mm/pgtable_32.c
===================================================================
--- linux-2.6.18.orig/arch/powerpc/mm/pgtable_32.c
+++ linux-2.6.18/arch/powerpc/mm/pgtable_32.c
@@ -26,7 +26,13 @@
 #include <linux/vmalloc.h>
 #include <linux/init.h>
 #include <linux/highmem.h>
+#include <linux/sched.h>
 
+#ifdef CONFIG_SMP
+#include <linux/rcupdate.h>
+#endif
+
+#include <asm/tlb.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/io.h>
@@ -48,7 +54,7 @@ int io_bat_index;
 
 extern char etext[], _stext[];
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_FSL_BOOKE)
 extern void hash_page_sync(void);
 #endif
 
@@ -79,6 +85,81 @@ extern unsigned long p_mapped_by_tlbcam(
 #define PGDIR_ORDER	0
 #endif
 
+#ifdef CONFIG_SMP
+struct pte_freelist_batch
+{
+	struct rcu_head	rcu;
+	unsigned int	index;
+	struct page *tables[0];
+};
+
+#define PTE_FREELIST_SIZE \
+	((PAGE_SIZE - sizeof(struct pte_freelist_batch)) \
+	  / sizeof(struct page *))
+
+DEFINE_PER_CPU(struct pte_freelist_batch *, pte_freelist_cur);
+
+static void pte_free_smp_sync(void *arg)
+{
+	/* Do nothing, just ensure we sync with all CPUs */
+}
+
+/* This is only called when we are critically out of memory
+ * (and fail to get a page in pte_free_tlb).
+ */
+static void pgtable_free_now(struct page *pte)
+{
+	smp_call_function(pte_free_smp_sync, NULL, 0, 1);
+
+	pte_free(pte);
+}
+
+static void pte_free_rcu_callback(struct rcu_head *head)
+{
+	struct pte_freelist_batch *batch =
+		container_of(head, struct pte_freelist_batch, rcu);
+	unsigned int i;
+
+	for (i = 0; i < batch->index; i++)
+		pte_free(batch->tables[i]);
+
+	free_page((unsigned long)batch);
+}
+
+static void pte_free_submit(struct pte_freelist_batch *batch)
+{
+	INIT_RCU_HEAD(&batch->rcu);
+	call_rcu(&batch->rcu, pte_free_rcu_callback);
+}
+
+void pgtable_free_tlb(struct mmu_gather *tlb, struct page *pte)
+{
+	cpumask_t local_cpumask = cpumask_of_cpu(tlb->cpu);
+	struct pte_freelist_batch **batchp = &per_cpu(pte_freelist_cur, tlb->cpu);
+
+	if (atomic_read(&tlb->mm->mm_users) < 2 ||
+	    cpus_equal(tlb->mm->cpu_vm_mask, local_cpumask)) {
+		pte_free(pte);
+		return;
+	}
+
+	if (*batchp == NULL) {
+		*batchp = (struct pte_freelist_batch *)__get_free_page(GFP_ATOMIC);
+		if (*batchp == NULL) {
+			pgtable_free_now(pte);
+			return;
+		}
+		(*batchp)->index = 0;
+	}
+	(*batchp)->tables[(*batchp)->index++] = pte;
+	if ((*batchp)->index == PTE_FREELIST_SIZE) {
+		pte_free_submit(*batchp);
+		*batchp = NULL;
+	}
+}
+
+#endif	/* CONFIG_SMP */
+
 pgd_t *pgd_alloc(struct mm_struct *mm)
 {
 	pgd_t *ret;
@@ -126,7 +207,7 @@ struct page *pte_alloc_one(struct mm_str
 
 void pte_free_kernel(pte_t *pte)
 {
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_FSL_BOOKE)
 	hash_page_sync();
 #endif
 	free_page((unsigned long)pte);
@@ -134,7 +215,7 @@ void pte_free_kernel(pte_t *pte)
 
 void pte_free(struct page *ptepage)
 {
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_FSL_BOOKE)
 	hash_page_sync();
 #endif
 	__free_page(ptepage);
Index: linux-2.6.18/include/asm-powerpc/pgalloc-32.h
===================================================================
--- linux-2.6.18.orig/include/asm-powerpc/pgalloc-32.h
+++ linux-2.6.18/include/asm-powerpc/pgalloc-32.h
@@ -34,7 +34,14 @@ extern struct page *pte_alloc_one(struct
 extern void pte_free_kernel(pte_t *pte);
 extern void pte_free(struct page *pte);
 
+#ifdef CONFIG_SMP
+extern void pgtable_free_tlb(struct mmu_gather *tlb, struct page *pte);
+
+#define __pte_free_tlb(tlb, pte)	pgtable_free_tlb(tlb, pte)
+
+#else
 #define __pte_free_tlb(tlb, pte)	pte_free((pte))
+#endif	/* CONFIG_SMP */
 
 #define check_pgt_cache()	do { } while (0)
 
Index: linux-2.6.18/include/asm-powerpc/reg_booke.h
===================================================================
--- linux-2.6.18.orig/include/asm-powerpc/reg_booke.h
+++ linux-2.6.18/include/asm-powerpc/reg_booke.h
@@ -388,12 +388,14 @@
 #define ICCR_CACHE	1		/* Cacheable */
 
 /* Bit definitions for L1CSR0. */
+#define L1CSR0_DCPE	0x00010000	/* Data Cache Parity Enable */
 #define L1CSR0_CLFC	0x00000100	/* Cache Lock Bits Flash Clear */
 #define L1CSR0_DCFI	0x00000002	/* Data Cache Flash Invalidate */
 #define L1CSR0_CFI	0x00000002	/* Cache Flash Invalidate */
 #define L1CSR0_DCE	0x00000001	/* Data Cache Enable */
 
 /* Bit definitions for L1CSR1. */
+#define L1CSR1_ICPE	0x00010000	/* Instr Cache Parity Enable */
 #define L1CSR1_ICLFR	0x00000100	/* Instr Cache Lock Bits Flash Reset */
 #define L1CSR1_ICFI	0x00000002	/* Instr Cache Flash Invalidate */
 #define L1CSR1_ICE	0x00000001	/* Instr Cache Enable */
Index: linux-2.6.18/mvl_patches/pro50-1015.c
===================================================================
--- /dev/null
+++ linux-2.6.18/mvl_patches/pro50-1015.c
@@ -0,0 +1,16 @@
+/*
+ * Author: MontaVista Software, Inc. <source@mvista.com>
+ *
+ * 2008 (c) MontaVista Software, Inc. This file is licensed under
+ * the terms of the GNU General Public License version 2. This program
+ * is licensed "as is" without any warranty of any kind, whether express
+ * or implied.
+ */
+#include <linux/init.h>
+#include <linux/mvl_patch.h>
+
+static __init int regpatch(void)
+{
+        return mvl_register_patch(1015);
+}
+module_init(regpatch);
EOF

    rv=0
    cat /tmp/mvl_patch_$$
    if [ "$?" != "0" ]; then
	# Patch had a hard error, return 2
	rv=2
    elif grep '^Hunk' ${TMPFILE}; then
	rv=1
    fi

    rm -f ${TMPFILE}
    return $rv
}

function options() {
    echo "Options are:"
    echo "  --force-unsupported - Force the patch to be applied even if the"
    echo "      patch is out of order or the current kernel is unsupported."
    echo "      Use of this option is strongly discouraged."
    echo "  --force-apply-fuzz - If the patch has fuzz, go ahead and apply"
    echo "      it anyway.  This can occur if the patch is applied to an"
    echo "      unsupported kernel or applied out of order or if you have"
    echo "      made your own modifications to the kernel.  Use with"
    echo "      caution."
    echo "  --remove - Remove the patch"
}


function checkpatchnum() {
    local level;

    if [ ! -e ${1} ]; then
	echo "${1} does not exist, make sure you are in the kernel" 1>&2
	echo "base directory" 1>&2
	exit 1;
    fi

    # Extract the current patch number from the lsp info file.
    level=`grep '#define LSP_.*PATCH_LEVEL' ${1} | sed 's/^.*\"\\(.*\\)\".*\$/\\1/'`
    if [ "a$level" = "a" ]; then
	echo "No patch level defined in ${1}, are you sure this is" 1>&2
	echo "a valid MVL kernel LSP?" 1>&2
	exit 1;
    fi

    expr $level + 0 >/dev/null 2>&1
    isnum=$?

    # Check if the kernel is supported
    if [ "$level" = "unsupported" ]; then
	echo "**Current kernel is unsupported by MontaVista due to patches"
	echo "  begin applied out of order."
	if [ $force_unsupported == 't' ]; then
	    echo "  Application is forced, applying patch anyway"
	    unsupported=t
	    fix_patch_level=f
	else
	    echo "  Patch application aborted.  Use --force-unsupported to"
	    echo "  force the patch to be applied, but the kernel will not"
	    echo "  be supported by MontaVista."
	    exit 1;
	fi

    # Check the patch number from the lspinfo file to make sure it is
    # a valid number
    elif [ $isnum = 2 ]; then
	echo "**Patch level from ${1} was not a valid number, " 1>&2
	echo "  are you sure this is a valid MVL kernel LSP?" 1>&2
	exit 1;

    # Check that this is the right patch number to be applied.
    elif [ `expr $level $3` ${4} ${2} ]; then
	echo "**Application of this patch is out of order and will cause the"
	echo "  kernel to be unsupported by MontaVista."
	if [ $force_unsupported == 't' ]; then
	    echo "  application is forced, applying patch anyway"
	    unsupported=t
	else
	    echo "  Patch application aborted.  Please get all the patches in"
	    echo "  proper order from MontaVista Zone and apply them in order"
	    echo "  If you really want to apply this patch, use"
	    echo "  --force-unsupported to force the patch to be applied, but"
	    echo "  the kernel will not be supported by MontaVista."
	    exit 1;
	fi
    fi
}

#
# Update the patch level in the file.  Note that we use patch to do
# this.  Certain weak version control systems don't take kindly to
# arbitrary changes directly to files, but do have a special version
# of "patch" that understands this.
#
function setpatchnum() {
    sed "s/^#define LSP_\(.*\)PATCH_LEVEL[ \t*]\"[0-9]*\".*$/#define LSP_\1PATCH_LEVEL \"${2}\"/" <${1} >/tmp/$$.tmp1
    diff -u ${1} /tmp/$$.tmp1 >/tmp/$$.tmp2
    rm /tmp/$$.tmp1
    sed "s/^+++ \/tmp\/$$.tmp1/+++ include\/linux\/lsppatchlevel.h/" </tmp/$$.tmp2 >/tmp/$$.tmp1
    rm /tmp/$$.tmp2
    patch -p0 </tmp/$$.tmp1
    rm /tmp/$$.tmp1
}

force_unsupported=f
force_apply_fuzz=""
unsupported=f
fix_patch_level=t
reverse=f
common_patchnum_diff='+ 1'
common_patchnum=$PATCHNUM
patch_extraopts=''

# Extract command line parameters.
while [ $# -gt 0 ]; do
    if [ "a$1" == 'a--force-unsupported' ]; then
	force_unsupported=t
    elif [ "a$1" == 'a--force-apply-fuzz' ]; then
	force_apply_fuzz=y
    elif [ "a$1" == 'a--remove' ]; then
	reverse=t
	common_patchnum_diff=''
	common_patchnum=`expr $PATCHNUM - 1`
	patch_extraopts='--reverse'
    else
	echo "'$1' is an invalid command line parameter."
	options
	exit 1
    fi
    shift
done

echo "Checking patch level"
checkpatchnum ${LSPINFO} ${PATCHNUM} "${common_patchnum_diff}" "-ne"

if ! dopatch -p1 --dry-run --force $patch_extraopts; then
    if [ $? = 2 ]; then
	echo -n "**Patch had errors, application aborted" 1>&2
	exit 1;
    fi

    # Patch has warnings
    clean_apply=${force_apply_fuzz}
    while [ "a$clean_apply" != 'ay' -a "a$clean_apply" != 'an' ]; do
	echo -n "**Patch did not apply cleanly.  Do you still want to apply? (y/n) > "
	read clean_apply
	clean_apply=`echo "$clean_apply" | tr '[:upper:]' '[:lower:]'`
    done
    if [ $clean_apply = 'n' ]; then
	exit 1;
    fi
fi

dopatch -p1 --force $patch_extraopts

if [ $fix_patch_level = 't' ]; then 
    if [ $unsupported = 't' ]; then
	common_patchnum="unsupported"
    fi

    setpatchnum ${LSPINFO} ${common_patchnum}
fi

# Move the patch file into the mvl_patches directory if we are not reversing
if [ $reverse != 't' ]; then 
    if echo $0 | grep '/' >/dev/null; then
	# Filename is a path, either absolute or from the current directory.
	srcfile=$0
    else
	# Filename is from the path
	for i in `echo $PATH | tr ':;' '  '`; do
	    if [ -e ${i}/$0 ]; then
		srcfile=${i}/$0
	    fi
	done
    fi

    fname=`basename ${srcfile}`
    diff -uN mvl_patches/${fname} ${srcfile} | (cd mvl_patches; patch)
fi

